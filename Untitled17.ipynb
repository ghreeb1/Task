{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO3+TSbTwAx9sEErD/rNlRW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import string\n","from collections import Counter\n","\n","class SimpleWikiScraper:\n","    def __init__(self):\n","        # Common English stopwords (avoiding NLTK dependency)\n","        self.stop_words = {\n","            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n","            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n","            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n","            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n","            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n","            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n","            'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after',\n","            'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n","            'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n","            'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',\n","            'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n","            'just', 'don', 'should', 'now'\n","        }\n","\n","    def scrape_wikipedia_page(self, url):\n","        \"\"\"\n","        Scrape Wikipedia page content using BeautifulSoup\n","        \"\"\"\n","        try:\n","            headers = {\n","                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","            }\n","            response = requests.get(url, headers=headers)\n","            response.raise_for_status()\n","\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","\n","            data = {\n","                'title': self.extract_title(soup),\n","                'content': self.extract_main_content(soup),\n","                'sections': self.extract_sections(soup),\n","                'references': self.extract_references(soup),\n","                'infobox': self.extract_infobox(soup)\n","            }\n","\n","            return data\n","\n","        except requests.RequestException as e:\n","            print(f\"Error scraping the page: {e}\")\n","            return None\n","\n","    def extract_title(self, soup):\n","        \"\"\"Extract page title\"\"\"\n","        title_element = soup.find('h1', {'class': 'firstHeading'})\n","        return title_element.get_text().strip() if title_element else \"No title found\"\n","\n","    def extract_main_content(self, soup):\n","        \"\"\"Extract main article content\"\"\"\n","        content_div = soup.find('div', {'class': 'mw-parser-output'})\n","        if not content_div:\n","            return \"No content found\"\n","\n","        # Remove unwanted elements\n","        for element in content_div(['table', 'div', 'sup', 'span']):\n","            element.decompose()\n","\n","        paragraphs = content_div.find_all('p')\n","        content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n","\n","        return content\n","\n","    def extract_sections(self, soup):\n","        \"\"\"Extract section headers and their content\"\"\"\n","        sections = {}\n","        headers = soup.find_all(['h2', 'h3', 'h4'])\n","\n","        for header in headers:\n","            section_title = header.get_text().strip()\n","            section_title = re.sub(r'\\[edit\\]', '', section_title).strip()\n","\n","            content = []\n","            current = header.find_next_sibling()\n","\n","            while current and current.name not in ['h2', 'h3', 'h4']:\n","                if current.name == 'p':\n","                    text = current.get_text().strip()\n","                    if text:\n","                        content.append(text)\n","                current = current.find_next_sibling()\n","\n","            sections[section_title] = ' '.join(content)\n","\n","        return sections\n","\n","    def extract_references(self, soup):\n","        \"\"\"Extract references/citations\"\"\"\n","        ref_list = soup.find('ol', {'class': 'references'})\n","        if not ref_list:\n","            return []\n","\n","        references = []\n","        for ref in ref_list.find_all('li'):\n","            ref_text = ref.get_text().strip()\n","            references.append(ref_text)\n","\n","        return references\n","\n","    def extract_infobox(self, soup):\n","        \"\"\"Extract infobox information\"\"\"\n","        infobox = soup.find('table', {'class': 'infobox'})\n","        if not infobox:\n","            return {}\n","\n","        info_data = {}\n","        rows = infobox.find_all('tr')\n","\n","        for row in rows:\n","            header = row.find('th')\n","            data = row.find('td')\n","\n","            if header and data:\n","                key = header.get_text().strip()\n","                value = data.get_text().strip()\n","                info_data[key] = value\n","\n","        return info_data\n","\n","    def clean_text(self, text):\n","        \"\"\"Basic text cleaning\"\"\"\n","        if not text:\n","            return \"\"\n","\n","        # Remove extra whitespace\n","        text = re.sub(r'\\s+', ' ', text)\n","\n","        # Convert to lowercase\n","        text = text.lower()\n","\n","        # Remove URLs\n","        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n","\n","        # Remove email addresses\n","        text = re.sub(r'\\S+@\\S+', '', text)\n","\n","        # Remove citations [1], [2], etc.\n","        text = re.sub(r'\\[\\d+\\]', '', text)\n","\n","        return text.strip()\n","\n","    def simple_tokenize(self, text):\n","        \"\"\"Simple word tokenization\"\"\"\n","        # Remove punctuation and split\n","        text = re.sub(r'[^\\w\\s]', ' ', text)\n","        words = text.split()\n","\n","        # Split sentences (simple approach)\n","        sentences = re.split(r'[.!?]+', text)\n","        sentences = [s.strip() for s in sentences if s.strip()]\n","\n","        return {\n","            'words': words,\n","            'sentences': sentences,\n","            'word_count': len(words),\n","            'sentence_count': len(sentences)\n","        }\n","\n","    def remove_stopwords(self, words):\n","        \"\"\"Remove stopwords from word list\"\"\"\n","        return [word for word in words if word.lower() not in self.stop_words and len(word) > 2]\n","\n","    def simple_stem(self, word):\n","        \"\"\"Very basic stemming (remove common suffixes)\"\"\"\n","        suffixes = ['ing', 'ed', 'er', 'est', 'ly', 's']\n","        word = word.lower()\n","\n","        for suffix in suffixes:\n","            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n","                return word[:-len(suffix)]\n","        return word\n","\n","    def get_word_frequency(self, words, top_n=20):\n","        \"\"\"Get word frequency distribution\"\"\"\n","        word_freq = Counter(words)\n","        return word_freq.most_common(top_n)\n","\n","    def extract_key_phrases(self, text, min_length=2, max_length=4):\n","        \"\"\"Extract key phrases (n-grams)\"\"\"\n","        words = self.simple_tokenize(text)['words']\n","        words = self.remove_stopwords(words)\n","\n","        phrases = []\n","        for i in range(len(words)):\n","            for length in range(min_length, min(max_length + 1, len(words) - i + 1)):\n","                phrase = ' '.join(words[i:i+length])\n","                phrases.append(phrase)\n","\n","        phrase_freq = Counter(phrases)\n","        return phrase_freq.most_common(15)\n","\n","    def analyze_text_stats(self, text):\n","        \"\"\"Basic text statistics\"\"\"\n","        tokens = self.simple_tokenize(text)\n","        words = [word for word in tokens['words'] if word.isalpha()]\n","\n","        stats = {\n","            'total_characters': len(text),\n","            'total_words': len(words),\n","            'total_sentences': tokens['sentence_count'],\n","            'avg_words_per_sentence': len(words) / max(tokens['sentence_count'], 1),\n","            'avg_chars_per_word': sum(len(word) for word in words) / max(len(words), 1),\n","            'unique_words': len(set(words)),\n","            'lexical_diversity': len(set(words)) / max(len(words), 1)\n","        }\n","\n","        return stats\n","\n","    def process_scraped_data(self, scraped_data):\n","        \"\"\"Apply NLP processing to scraped data\"\"\"\n","        if not scraped_data:\n","            return None\n","\n","        processed_data = {\n","            'title': scraped_data['title'],\n","            'original_content': scraped_data['content'],\n","            'sections': scraped_data['sections'],\n","            'references_count': len(scraped_data['references']),\n","            'infobox': scraped_data['infobox']\n","        }\n","\n","        # Clean the main content\n","        cleaned_content = self.clean_text(scraped_data['content'])\n","        processed_data['cleaned_content'] = cleaned_content\n","\n","        # Tokenization\n","        tokens = self.simple_tokenize(cleaned_content)\n","        processed_data['tokenization'] = tokens\n","\n","        # Remove non-alphabetic tokens and stopwords\n","        words = [word for word in tokens['words'] if word.isalpha()]\n","        filtered_words = self.remove_stopwords(words)\n","\n","        processed_data['filtered_words'] = filtered_words\n","        processed_data['filtered_word_count'] = len(filtered_words)\n","\n","        # Simple stemming\n","        stemmed_words = [self.simple_stem(word) for word in filtered_words]\n","        processed_data['stemmed_words'] = stemmed_words\n","\n","        # Text statistics\n","        text_stats = self.analyze_text_stats(cleaned_content)\n","        processed_data['text_statistics'] = text_stats\n","\n","        # Word frequency analysis\n","        word_freq = self.get_word_frequency(filtered_words, top_n=25)\n","        processed_data['word_frequency'] = word_freq\n","\n","        # Key phrases extraction\n","        key_phrases = self.extract_key_phrases(cleaned_content)\n","        processed_data['key_phrases'] = key_phrases\n","\n","        # Process sections separately\n","        processed_sections = {}\n","        for section_title, section_content in scraped_data['sections'].items():\n","            if section_content.strip():\n","                cleaned_section = self.clean_text(section_content)\n","                section_tokens = self.simple_tokenize(cleaned_section)\n","                section_words = [word for word in section_tokens['words'] if word.isalpha()]\n","                section_filtered = self.remove_stopwords(section_words)\n","\n","                processed_sections[section_title] = {\n","                    'word_count': len(section_words),\n","                    'filtered_word_count': len(section_filtered),\n","                    'top_words': self.get_word_frequency(section_filtered, top_n=10),\n","                    'key_phrases': self.extract_key_phrases(cleaned_section, min_length=2, max_length=3)[:5]\n","                }\n","\n","        processed_data['processed_sections'] = processed_sections\n","\n","        return processed_data\n","\n","    def display_results(self, processed_data):\n","        \"\"\"Display the processing results\"\"\"\n","        if not processed_data:\n","            print(\"No data to display\")\n","            return\n","\n","        print(\"=\"*60)\n","        print(f\"WIKIPEDIA SCRAPING AND NLP PROCESSING RESULTS\")\n","        print(\"=\"*60)\n","        print(f\"Title: {processed_data['title']}\")\n","        print(f\"References Count: {processed_data['references_count']}\")\n","\n","        stats = processed_data['text_statistics']\n","        print(f\"\\nTEXT STATISTICS:\")\n","        print(f\"- Total Characters: {stats['total_characters']:,}\")\n","        print(f\"- Total Words: {stats['total_words']:,}\")\n","        print(f\"- Total Sentences: {stats['total_sentences']:,}\")\n","        print(f\"- Average Words per Sentence: {stats['avg_words_per_sentence']:.1f}\")\n","        print(f\"- Average Characters per Word: {stats['avg_chars_per_word']:.1f}\")\n","        print(f\"- Unique Words: {stats['unique_words']:,}\")\n","        print(f\"- Lexical Diversity: {stats['lexical_diversity']:.3f}\")\n","\n","        print(\"\\n\" + \"=\"*40)\n","        print(\"TOP 20 MOST FREQUENT WORDS:\")\n","        print(\"=\"*40)\n","        for word, freq in processed_data['word_frequency'][:20]:\n","            print(f\"{word:<15}: {freq:>3}\")\n","\n","        print(\"\\n\" + \"=\"*40)\n","        print(\"TOP KEY PHRASES:\")\n","        print(\"=\"*40)\n","        for phrase, freq in processed_data['key_phrases'][:15]:\n","            print(f\"{phrase:<25}: {freq:>3}\")\n","\n","        print(\"\\n\" + \"=\"*40)\n","        print(\"SECTION ANALYSIS (Top 5 sections):\")\n","        print(\"=\"*40)\n","        for section, data in list(processed_data['processed_sections'].items())[:5]:\n","            if section.strip():\n","                print(f\"\\nüìñ {section}\")\n","                print(f\"   Words: {data['word_count']}, Filtered: {data['filtered_word_count']}\")\n","                print(f\"   Top Words: {', '.join([word for word, freq in data['top_words'][:5]])}\")\n","                if data['key_phrases']:\n","                    print(f\"   Key Phrases: {', '.join([phrase for phrase, freq in data['key_phrases'][:3]])}\")\n","\n","        if processed_data['infobox']:\n","            print(\"\\n\" + \"=\"*40)\n","            print(\"INFOBOX DATA:\")\n","            print(\"=\"*40)\n","            for key, value in list(processed_data['infobox'].items())[:5]:\n","                print(f\"{key}: {value[:80]}{'...' if len(value) > 80 else ''}\")\n","\n","        print(\"\\n\" + \"=\"*40)\n","        print(\"SAMPLE CLEANED TEXT (First 300 chars):\")\n","        print(\"=\"*40)\n","        print(processed_data['cleaned_content'][:300] + \"...\")\n","\n","# Usage example\n","def main():\n","    \"\"\"Main function to demonstrate the scraper\"\"\"\n","    url = \"https://en.wikipedia.org/wiki/Machine_learning\"\n","\n","    scraper = SimpleWikiScraper()\n","\n","    print(\"üîÑ Scraping Wikipedia page...\")\n","    scraped_data = scraper.scrape_wikipedia_page(url)\n","\n","    if scraped_data:\n","        print(\"üîÑ Applying NLP processing...\")\n","        processed_data = scraper.process_scraped_data(scraped_data)\n","\n","        if processed_data:\n","            scraper.display_results(processed_data)\n","            print(\"\\n‚úÖ Processing completed successfully!\")\n","        else:\n","            print(\"‚ùå Failed to process the data\")\n","    else:\n","        print(\"‚ùå Failed to scrape the page\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x76uKYQOTWWy","executionInfo":{"status":"ok","timestamp":1754734383604,"user_tz":-180,"elapsed":584,"user":{"displayName":"Mohamed Khaled","userId":"14460568985060967945"}},"outputId":"369f94b4-206f-43c2-a4cd-67b45cfcd3e3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["üîÑ Scraping Wikipedia page...\n","üîÑ Applying NLP processing...\n","============================================================\n","WIKIPEDIA SCRAPING AND NLP PROCESSING RESULTS\n","============================================================\n","Title: Machine learning\n","References Count: 0\n","\n","TEXT STATISTICS:\n","- Total Characters: 54,413\n","- Total Words: 8,079\n","- Total Sentences: 1\n","- Average Words per Sentence: 8079.0\n","- Average Characters per Word: 5.6\n","- Unique Words: 1,990\n","- Lexical Diversity: 0.246\n","\n","========================================\n","TOP 20 MOST FREQUENT WORDS:\n","========================================\n","learning       : 207\n","machine        : 116\n","data           :  98\n","from           :  53\n","model          :  49\n","training       :  48\n","algorithms     :  43\n","used           :  34\n","set            :  32\n","models         :  31\n","artificial     :  27\n","systems        :  26\n","based          :  25\n","algorithm      :  25\n","methods        :  24\n","field          :  21\n","also           :  21\n","classification :  21\n","neural         :  20\n","supervised     :  19\n","\n","========================================\n","TOP KEY PHRASES:\n","========================================\n","machine learning         : 110\n","learning algorithms      :  25\n","neural networks          :  12\n","supervised learning      :  11\n","training data            :  10\n","data mining              :   9\n","machine learning algorithms:   8\n","artificial neurons       :   8\n","reinforcement learning   :   8\n","artificial intelligence  :   7\n","from data                :   7\n","deep learning            :   7\n","unsupervised learning    :   7\n","learning algorithm       :   7\n","data set                 :   7\n","\n","========================================\n","SECTION ANALYSIS (Top 5 sections):\n","========================================\n","\n","========================================\n","SAMPLE CLEANED TEXT (First 300 chars):\n","========================================\n","machine learning (ml) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. within a subdiscipline in machine learning, advances i...\n","\n","‚úÖ Processing completed successfully!\n"]}]}]}